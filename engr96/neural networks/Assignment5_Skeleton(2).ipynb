{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfT61medTBY6"
      },
      "source": [
        "# Homework 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JSocVUB5H3nw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "i1bPMoyyTRSB"
      },
      "source": [
        "This homework is designed to help build up some of the pieces you will use going forward when making neural networks.\n",
        "\n",
        "Let's imagine we are building a classifier for images. These images will be given to us as grayscale 3x3 images. (These would obviously be VERY simple images, this is just for tutorial reasons.) These images can either be bears üêª, dogs üêï, cats üêà, or fish üêü.\n",
        "\n",
        "First a few questions to think about:\n",
        "1. How many inputs does our neural network have?\n",
        "2. How should we change our input to make it easier to feed to the neural network?\n",
        "3. How many outputs does our neural network have?\n",
        "4. If our neural network does not use hidden layers, what would the dimensions of the array that represent our transformation from input to output be?\n",
        "5. What does this say about the dimensions of the arrays that make up all of our stages?\n",
        "\n",
        "üì∞ Task 1: Convert the 3x3 tensor into a 1x9 tensor which will be a better input for our neural network. First do this with the individual tensor (in1), then try to do it with the list of tensors (our \"training data\"). Then append the flattened in1 to the flattened training data (making it the 100th piece of data).\n",
        "\n",
        "Hint: consider using one of the following:\n",
        "* torch.flatten\n",
        "* torch.reshape\n",
        "* torch.cat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eR5x6DE7H5vr"
      },
      "outputs": [],
      "source": [
        "#3x3 tensor\n",
        "\n",
        "np.random.seed(0)\n",
        "in1_numpy = np.random.randint(256, size=(3,3))\n",
        "in1 = torch.tensor(in1_numpy, dtype=torch.float)\n",
        "\n",
        "in99_numpy = np.random.randint(256, size=(99, 3, 3))\n",
        "in99 = torch.tensor(in99_numpy, dtype=torch.float)\n",
        "\n",
        "picture_labels_numpy = np.random.randint(4, size=(100))\n",
        "picture_labels = torch.tensor(picture_labels_numpy, dtype=torch.float)\n",
        "\n",
        "########################\n",
        "## YOUR CODE STARTS HERE\n",
        "########################\n",
        "\n",
        "# Task 1: Flatten in1 to be a (1x9) tensor, then flatten training_data to a (99x9) tensor, then concatenate both into a (100x9) tensor\n",
        "in1_flattened = in1.reshape([1,9])\n",
        "training_data_flattened = in99.reshape([99, 9])\n",
        "\n",
        "final_data = torch.cat((in1_flattened, training_data_flattened), 0)\n",
        "\n",
        "########################\n",
        "## YOUR CODE ENDS HERE\n",
        "########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRNgPNUjd4W7"
      },
      "source": [
        "üöÜ Task 2: Make a simple neural network with one hidden layer for your data. The hidden layer should have 5 neurons on it. You can store your weights in two matrices, and initialize them to 1s or random numbers. Send their input through the hidden and output layers to get your final output.\n",
        "\n",
        "Things to consider:\n",
        "\n",
        "1. Why do we use matrices to store our weights?\n",
        "\n",
        "2. What will the dimensions of our matrices be?\n",
        "\n",
        "Hints:\n",
        "\n",
        "1. You can use torch.rand((R,C)) to initialize a matrix of random numbers.\n",
        "\n",
        "2. For simplicity, you do not need to consider biases for this problem, just think about weights.\n",
        "\n",
        "3. PyTorch has a matrix multiplication function called torch.matmul(m1, m2).\n",
        "\n",
        "4. You can use the ReLU activation function by calling F.relu(h1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3Q7k3ag1dxRP"
      },
      "outputs": [],
      "source": [
        "########################\n",
        "## YOUR CODE STARTS HERE\n",
        "########################\n",
        "weights1 = torch.rand((9,5))\n",
        "weights2 = torch.rand((5, 4))\n",
        "\n",
        "h1 = torch.matmul(final_data, weights1)  # Matrix multiplication between input data and first set of weights\n",
        "h1_activated = F.relu(h1)  # Activation function (ReLU) applied to the hidden layer's output\n",
        "\n",
        "# Hidden layer to output\n",
        "output = torch.matmul(h1_activated, weights2)  # Matrix multiplication\n",
        "\n",
        "\n",
        "########################\n",
        "## YOUR CODE ENDS HERE\n",
        "########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO5pxir5hdGK"
      },
      "source": [
        "Your output should now be four numbers, each of which are unbounded. We would prefer to have four bounded numbers, so we must apply the sigmoid function to each one.\n",
        "\n",
        "üßÆ Task 3: Use the sigmoid function to make all elements of your output between zero and one. Use .shape to ensure that your output is the expected size.\n",
        "\n",
        "Note: the torch.sigmoid() function is actualy an alias of the torch.special.expit() function, so try searching for both if one doesn't work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "k9ejynAQj5JT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([100, 4])\n"
          ]
        }
      ],
      "source": [
        "########################\n",
        "## YOUR CODE STARTS HERE\n",
        "########################\n",
        "sigmoid_output = torch.sigmoid(output)\n",
        "print(sigmoid_output.shape)\n",
        "\n",
        "########################\n",
        "## YOUR CODE ENDS HERE\n",
        "########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqj2FomSj8HO"
      },
      "source": [
        "Now we come across a common, but ultimately easy-to-solve problem in neural networks: how we should represent the output. Currently our output is four numbers between zero and one. We can convert these to all zero and a single one value if we want to produce an actual guess. This is called a one-hot encoding. However, as defined in the first code block, our actual picture label is only a single value for each image (thus a tensor of 100 values for the 100 images).This is called an integer encoding.\n",
        "\n",
        "üì≠ Task 4: Use a pytorch function (or make your own) that converts from integer encoding to one hot encoding to convert the picture labels to one hot encoding. Then compute the log-loss of your estimated output and the actual output.\n",
        "\n",
        "Hint: the following may be helpful: \\\n",
        "*   torch.nn.functional.one_hot\n",
        "*   torch.nn.functional.nll_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "########################\n",
        "## YOUR CODE STARTS HERE\n",
        "########################\n",
        "# Convert integer labels to one-hot encoding\n",
        "# Assuming the maximum label value is less than 4 since the network outputs 4 values\n",
        "num_classes = 4  # Number of output classes\n",
        "one_hot_labels = F.one_hot(picture_labels.to(torch.int64), num_classes=num_classes)\n",
        "\n",
        "# Convert network outputs to probabilities\n",
        "probabilities = F.softmax(sigmoid_output, dim=1)\n",
        "\n",
        "# Compute log probabilities\n",
        "log_probabilities = torch.log(probabilities + 1e-5)  # Adding a small value to prevent log(0)\n",
        "\n",
        "# Compute the log-loss\n",
        "# nll_loss expects inputs in log form; however, it is designed for use with log_softmax, so we'll use a workaround\n",
        "# Convert one-hot labels to probabilities by multiplying with log_probabilities and summing for each sample\n",
        "loss = -torch.sum(one_hot_labels * log_probabilities) / one_hot_labels.shape[0]\n",
        "########################\n",
        "## YOUR CODE ENDS HERE\n",
        "########################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncxlFDemI94n"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Implemented a simple neural network in PyTorch for processing 100 images. The network has an input layer, one hidden layer with 5 neurons, and an output layer producing 4 values per input.\n",
        "\n",
        "Steps\n",
        "Preprocessing: Flattened and concatenated image tensors to create a 100x9 input tensor.\n",
        "Network Construction: Initialized weights for connections between layers. Used ReLU for the hidden layer and sigmoid to bound output values between 0 and 1.\n",
        "Output Processing: Applied softmax to convert network outputs to probabilities.\n",
        "One-Hot Encoding and Loss: Converted integer labels to one-hot encoding and computed log-loss between predictions and actual labels.\n",
        "Conclusion\n",
        "This process involved data preparation, network setup, prediction adjustment, and accuracy assessment through log-loss, illustrating basic neural network operations in PyTorch.\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
